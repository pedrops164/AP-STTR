{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaCrhPsRpCJ6",
        "outputId": "9c8eb673-ba1f-47e1-8089-70a63bf58264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'STTR'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 48 (delta 6), reused 37 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (48/48), 2.14 MiB | 13.76 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/researchmm/STTR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd STTR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IicL4YzapYBg",
        "outputId": "3b87fe14-fc0d-4b81-f029-9ee4bd1e47be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/STTR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main_style_transfer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUCHrevIpakf",
        "outputId": "d78014f0-cc86-49ee-ec9a-4fb95490f44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "git:\n",
            "  sha: ff52ccfc3a491209eb5b18260e5fba24052b17ab, status: has uncommited changes, branch: main\n",
            "\n",
            "Namespace(aux_loss=True, backbone='resnet50', batch_size=1, bbox_loss_coef=5, cbackbone_layer=2, clip_max_norm=0.1, coco_panoptic_path=None, coco_path=None, content_loss_coef=0.01, dataset_file='demo', dec_layers=6, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dnorm=True, dropout=0.1, enc_layers=6, enorm=True, eos_coef=0.1, epochs=300, eval=False, fold_k=5, fold_stride=6, frozen_weights=None, giou_loss_coef=2, hidden_dim=256, img_size=408, lr=1e-05, lr_backbone=1e-05, lr_drop=200, mask_loss_coef=1, masks=False, model_pre=False, model_type='nofold', nheads=8, num_queries=100, num_workers=1, output_dir='outputs', position_embedding='sine', pre_norm=False, remove_difficult=False, resume='', sbackbone_layer=4, seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, style_loss_coef=0.002, tnorm=True, tv_loss_coef=0, weight_decay=0.0001, wikiart_path=None, world_size=1)\n",
            "ISTT_NOFOLD(\n",
            "  (transformer): Transformer(\n",
            "    (encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (backbone_content): Backbone_50(\n",
            "    (body): IntermediateLayerGetter(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): FrozenBatchNorm2d()\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (backbone_style): Backbone_50(\n",
            "    (body): IntermediateLayerGetter(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): FrozenBatchNorm2d()\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (position_embedding): PositionEmbeddingSine()\n",
            "  (input_proj_c): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (input_proj_s): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (output_proj): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (tail): Sequential(\n",
            "    (0): ResBlock(\n",
            "      (net): Sequential(\n",
            "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.01)\n",
            "        (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.01)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (7): LeakyReLU(negative_slope=0.01)\n",
            "        (8): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (1): Upsample(scale_factor=2.0, mode=bilinear)\n",
            "    (2): ReflectionPad2d((1, 1, 1, 1))\n",
            "    (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ResBlock(\n",
            "      (net): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.01)\n",
            "        (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.01)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (7): LeakyReLU(negative_slope=0.01)\n",
            "        (8): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (5): Upsample(scale_factor=2.0, mode=bilinear)\n",
            "    (6): ReflectionPad2d((1, 1, 1, 1))\n",
            "    (7): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (8): ResBlock(\n",
            "      (net): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.01)\n",
            "        (2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.01)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (7): LeakyReLU(negative_slope=0.01)\n",
            "        (8): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (9): Upsample(scale_factor=2.0, mode=bilinear)\n",
            "    (10): ReflectionPad2d((1, 1, 1, 1))\n",
            "    (11): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1))\n",
            "  )\n",
            ")\n",
            "number of params: 45197827\n",
            "len(content_images),len(style_images): 3 5\n",
            "len(content_images),len(style_images): 3 5\n",
            "Start training\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3455: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "Epoch: [0]  [ 0/15]  eta: 0:01:34  lr: 0.000010  loss: 0.2292 (0.2292)  loss_content: 0.1309 (0.1309)  loss_style: 0.0983 (0.0983)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 13.0942 (13.0942)  loss_style_unscaled: 49.1412 (49.1412)  loss_tv_unscaled: 3523.7615 (3523.7615)  time: 6.3216  data: 0.1576  max mem: 6661\n",
            "Epoch: [0]  [14/15]  eta: 0:00:00  lr: 0.000010  loss: 0.1832 (0.1926)  loss_content: 0.1276 (0.1213)  loss_style: 0.0569 (0.0713)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.7621 (12.1283)  loss_style_unscaled: 28.4252 (35.6360)  loss_tv_unscaled: 3363.0884 (3096.6021)  time: 0.9514  data: 0.0140  max mem: 7253\n",
            "Epoch: [0] Total time: 0:00:14 (0.9592 s / it)\n",
            "Averaged stats:lr: 0.000010  loss: 0.1832 (0.1926)  loss_content: 0.1276 (0.1213)  loss_style: 0.0569 (0.0713)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.7621 (12.1283)  loss_style_unscaled: 28.4252 (35.6360)  loss_tv_unscaled: 3363.0884 (3096.6021)\n",
            "Test:  [ 0/15]  eta: 0:00:08  loss: 0.2342 (0.2342)  loss_content: 0.0887 (0.0887)  loss_style: 0.1454 (0.1454)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 8.8733 (8.8733)  loss_style_unscaled: 72.7138 (72.7138)  loss_tv_unscaled: 1.7586 (1.7586)  time: 0.5424  data: 0.1363  max mem: 7253\n",
            "Test:  [14/15]  eta: 0:00:00  loss: 0.2001 (0.2095)  loss_content: 0.1093 (0.1052)  loss_style: 0.0908 (0.1043)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 10.9269 (10.5206)  loss_style_unscaled: 45.3953 (52.1716)  loss_tv_unscaled: 1.9737 (2.0708)  time: 0.2035  data: 0.0126  max mem: 7253\n",
            "Test: Total time: 0:00:03 (0.2087 s / it)\n",
            "Averaged stats: loss: 0.2001 (0.2095)  loss_content: 0.1093 (0.1052)  loss_style: 0.0908 (0.1043)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 10.9269 (10.5206)  loss_style_unscaled: 45.3953 (52.1716)  loss_tv_unscaled: 1.9737 (2.0708)\n",
            "Epoch: [1]  [ 0/15]  eta: 0:00:37  lr: 0.000010  loss: 0.1770 (0.1770)  loss_content: 0.1273 (0.1273)  loss_style: 0.0497 (0.0497)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.7319 (12.7319)  loss_style_unscaled: 24.8432 (24.8432)  loss_tv_unscaled: 2265.5874 (2265.5874)  time: 2.5156  data: 0.1065  max mem: 7253\n",
            "Epoch: [1]  [14/15]  eta: 0:00:00  lr: 0.000010  loss: 0.1823 (0.1909)  loss_content: 0.1241 (0.1180)  loss_style: 0.0577 (0.0729)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.4126 (11.7988)  loss_style_unscaled: 28.8715 (36.4617)  loss_tv_unscaled: 3212.5889 (2916.0941)  time: 0.7205  data: 0.0124  max mem: 7253\n",
            "Epoch: [1] Total time: 0:00:10 (0.7258 s / it)\n",
            "Averaged stats:lr: 0.000010  loss: 0.1823 (0.1909)  loss_content: 0.1241 (0.1180)  loss_style: 0.0577 (0.0729)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.4126 (11.7988)  loss_style_unscaled: 28.8715 (36.4617)  loss_tv_unscaled: 3212.5889 (2916.0941)\n",
            "Test:  [ 0/15]  eta: 0:00:10  loss: 0.2296 (0.2296)  loss_content: 0.0896 (0.0896)  loss_style: 0.1400 (0.1400)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 8.9569 (8.9569)  loss_style_unscaled: 70.0230 (70.0230)  loss_tv_unscaled: 25.3645 (25.3645)  time: 0.6722  data: 0.1925  max mem: 7253\n",
            "Test:  [14/15]  eta: 0:00:00  loss: 0.1980 (0.2059)  loss_content: 0.1106 (0.1064)  loss_style: 0.0874 (0.0995)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.0553 (10.6399)  loss_style_unscaled: 43.6788 (49.7266)  loss_tv_unscaled: 24.7989 (42.7835)  time: 0.2135  data: 0.0164  max mem: 7253\n",
            "Test: Total time: 0:00:03 (0.2189 s / it)\n",
            "Averaged stats: loss: 0.1980 (0.2059)  loss_content: 0.1106 (0.1064)  loss_style: 0.0874 (0.0995)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.0553 (10.6399)  loss_style_unscaled: 43.6788 (49.7266)  loss_tv_unscaled: 24.7989 (42.7835)\n",
            "Epoch: [2]  [ 0/15]  eta: 0:02:11  lr: 0.000010  loss: 0.1754 (0.1754)  loss_content: 0.1254 (0.1254)  loss_style: 0.0500 (0.0500)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.5417 (12.5417)  loss_style_unscaled: 25.0023 (25.0023)  loss_tv_unscaled: 2148.7412 (2148.7412)  time: 8.7753  data: 0.1175  max mem: 7253\n",
            "Epoch: [2]  [14/15]  eta: 0:00:01  lr: 0.000010  loss: 0.1804 (0.1896)  loss_content: 0.1224 (0.1167)  loss_style: 0.0581 (0.0729)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.2406 (11.6670)  loss_style_unscaled: 29.0439 (36.4644)  loss_tv_unscaled: 3147.1379 (2835.2618)  time: 1.1319  data: 0.0116  max mem: 7256\n",
            "Epoch: [2] Total time: 0:00:17 (1.1372 s / it)\n",
            "Averaged stats:lr: 0.000010  loss: 0.1804 (0.1896)  loss_content: 0.1224 (0.1167)  loss_style: 0.0581 (0.0729)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.2406 (11.6670)  loss_style_unscaled: 29.0439 (36.4644)  loss_tv_unscaled: 3147.1379 (2835.2618)\n",
            "Test:  [ 0/15]  eta: 0:00:11  loss: 0.2173 (0.2173)  loss_content: 0.0924 (0.0924)  loss_style: 0.1249 (0.1249)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 9.2433 (9.2433)  loss_style_unscaled: 62.4540 (62.4540)  loss_tv_unscaled: 276.2907 (276.2907)  time: 0.7605  data: 0.2282  max mem: 7256\n",
            "Test:  [14/15]  eta: 0:00:00  loss: 0.1902 (0.1981)  loss_content: 0.1149 (0.1101)  loss_style: 0.0758 (0.0880)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.4949 (11.0134)  loss_style_unscaled: 37.8926 (43.9869)  loss_tv_unscaled: 297.8436 (459.8227)  time: 0.2434  data: 0.0218  max mem: 7256\n",
            "Test: Total time: 0:00:03 (0.2487 s / it)\n",
            "Averaged stats: loss: 0.1902 (0.1981)  loss_content: 0.1149 (0.1101)  loss_style: 0.0758 (0.0880)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.4949 (11.0134)  loss_style_unscaled: 37.8926 (43.9869)  loss_tv_unscaled: 297.8436 (459.8227)\n",
            "Epoch: [3]  [ 0/15]  eta: 0:00:43  lr: 0.000010  loss: 0.1428 (0.1428)  loss_content: 0.1019 (0.1019)  loss_style: 0.0409 (0.0409)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 10.1941 (10.1941)  loss_style_unscaled: 20.4497 (20.4497)  loss_tv_unscaled: 3132.3357 (3132.3357)  time: 2.8769  data: 0.1165  max mem: 7256\n",
            "Epoch: [3]  [14/15]  eta: 0:00:00  lr: 0.000010  loss: 0.1799 (0.1888)  loss_content: 0.1219 (0.1163)  loss_style: 0.0579 (0.0725)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.1927 (11.6347)  loss_style_unscaled: 28.9379 (36.2291)  loss_tv_unscaled: 3119.8799 (2823.0573)  time: 0.7392  data: 0.0123  max mem: 7256\n",
            "Epoch: [3] Total time: 0:00:11 (0.7447 s / it)\n",
            "Averaged stats:lr: 0.000010  loss: 0.1799 (0.1888)  loss_content: 0.1219 (0.1163)  loss_style: 0.0579 (0.0725)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.1927 (11.6347)  loss_style_unscaled: 28.9379 (36.2291)  loss_tv_unscaled: 3119.8799 (2823.0573)\n",
            "Test:  [ 0/15]  eta: 0:00:08  loss: 0.2142 (0.2142)  loss_content: 0.0971 (0.0971)  loss_style: 0.1171 (0.1171)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 9.7101 (9.7101)  loss_style_unscaled: 58.5440 (58.5440)  loss_tv_unscaled: 577.6726 (577.6726)  time: 0.5598  data: 0.1322  max mem: 7256\n",
            "Test:  [14/15]  eta: 0:00:00  loss: 0.1870 (0.1949)  loss_content: 0.1174 (0.1128)  loss_style: 0.0693 (0.0821)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.7426 (11.2783)  loss_style_unscaled: 34.6622 (41.0524)  loss_tv_unscaled: 659.0657 (700.0334)  time: 0.2113  data: 0.0130  max mem: 7256\n",
            "Test: Total time: 0:00:03 (0.2170 s / it)\n",
            "Averaged stats: loss: 0.1870 (0.1949)  loss_content: 0.1174 (0.1128)  loss_style: 0.0693 (0.0821)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.7426 (11.2783)  loss_style_unscaled: 34.6622 (41.0524)  loss_tv_unscaled: 659.0657 (700.0334)\n",
            "Epoch: [4]  [ 0/15]  eta: 0:00:43  lr: 0.000010  loss: 0.1416 (0.1416)  loss_content: 0.1008 (0.1008)  loss_style: 0.0408 (0.0408)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 10.0785 (10.0785)  loss_style_unscaled: 20.4227 (20.4227)  loss_tv_unscaled: 3153.5347 (3153.5347)  time: 2.8794  data: 0.1068  max mem: 7256\n",
            "Epoch: [4]  [14/15]  eta: 0:00:00  lr: 0.000010  loss: 0.1795 (0.1883)  loss_content: 0.1218 (0.1160)  loss_style: 0.0574 (0.0723)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.1754 (11.6023)  loss_style_unscaled: 28.7041 (36.1615)  loss_tv_unscaled: 3171.2808 (2867.6910)  time: 0.7344  data: 0.0107  max mem: 7256\n",
            "Epoch: [4] Total time: 0:00:11 (0.7399 s / it)\n",
            "Averaged stats:lr: 0.000010  loss: 0.1795 (0.1883)  loss_content: 0.1218 (0.1160)  loss_style: 0.0574 (0.0723)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.1754 (11.6023)  loss_style_unscaled: 28.7041 (36.1615)  loss_tv_unscaled: 3171.2808 (2867.6910)\n",
            "Test:  [ 0/15]  eta: 0:00:09  loss: 0.2134 (0.2134)  loss_content: 0.0945 (0.0945)  loss_style: 0.1189 (0.1189)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 9.4515 (9.4515)  loss_style_unscaled: 59.4654 (59.4654)  loss_tv_unscaled: 715.9740 (715.9740)  time: 0.6085  data: 0.1391  max mem: 7256\n",
            "Test:  [14/15]  eta: 0:00:00  loss: 0.1890 (0.1945)  loss_content: 0.1171 (0.1113)  loss_style: 0.0704 (0.0832)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.7094 (11.1329)  loss_style_unscaled: 35.1824 (41.6027)  loss_tv_unscaled: 602.6791 (647.3110)  time: 0.2395  data: 0.0160  max mem: 7256\n",
            "Test: Total time: 0:00:03 (0.2451 s / it)\n",
            "Averaged stats: loss: 0.1890 (0.1945)  loss_content: 0.1171 (0.1113)  loss_style: 0.0704 (0.0832)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.7094 (11.1329)  loss_style_unscaled: 35.1824 (41.6027)  loss_tv_unscaled: 602.6791 (647.3110)\n",
            "Epoch: [5]  [ 0/15]  eta: 0:00:42  lr: 0.000010  loss: 0.1784 (0.1784)  loss_content: 0.1212 (0.1212)  loss_style: 0.0573 (0.0573)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.1166 (12.1166)  loss_style_unscaled: 28.6255 (28.6255)  loss_tv_unscaled: 3298.2388 (3298.2388)  time: 2.8609  data: 0.1344  max mem: 7256\n",
            "Epoch: [5]  [14/15]  eta: 0:00:00  lr: 0.000010  loss: 0.1784 (0.1877)  loss_content: 0.1211 (0.1156)  loss_style: 0.0573 (0.0721)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.1061 (11.5573)  loss_style_unscaled: 28.6255 (36.0561)  loss_tv_unscaled: 3206.6506 (2923.5485)  time: 0.7445  data: 0.0134  max mem: 7256\n",
            "Epoch: [5] Total time: 0:00:11 (0.7526 s / it)\n",
            "Averaged stats:lr: 0.000010  loss: 0.1784 (0.1877)  loss_content: 0.1211 (0.1156)  loss_style: 0.0573 (0.0721)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.1061 (11.5573)  loss_style_unscaled: 28.6255 (36.0561)  loss_tv_unscaled: 3206.6506 (2923.5485)\n",
            "Test:  [ 0/15]  eta: 0:00:09  loss: 0.2161 (0.2161)  loss_content: 0.0932 (0.0932)  loss_style: 0.1229 (0.1229)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 9.3191 (9.3191)  loss_style_unscaled: 61.4658 (61.4658)  loss_tv_unscaled: 429.4956 (429.4956)  time: 0.6109  data: 0.1507  max mem: 7256\n",
            "Test:  [14/15]  eta: 0:00:00  loss: 0.1883 (0.1956)  loss_content: 0.1147 (0.1094)  loss_style: 0.0736 (0.0862)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.4733 (10.9388)  loss_style_unscaled: 36.7834 (43.0893)  loss_tv_unscaled: 418.3952 (425.5554)  time: 0.2154  data: 0.0135  max mem: 7256\n",
            "Test: Total time: 0:00:03 (0.2228 s / it)\n",
            "Averaged stats: loss: 0.1883 (0.1956)  loss_content: 0.1147 (0.1094)  loss_style: 0.0736 (0.0862)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 11.4733 (10.9388)  loss_style_unscaled: 36.7834 (43.0893)  loss_tv_unscaled: 418.3952 (425.5554)\n",
            "Epoch: [6]  [ 0/15]  eta: 0:00:50  lr: 0.000010  loss: 0.1702 (0.1702)  loss_content: 0.1203 (0.1203)  loss_style: 0.0499 (0.0499)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.0316 (12.0316)  loss_style_unscaled: 24.9585 (24.9585)  loss_tv_unscaled: 3271.2959 (3271.2959)  time: 3.3717  data: 0.1897  max mem: 7256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo_sttr_image_ACCV.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcanNfQ5fgky",
        "outputId": "f1518ced-d827-4e23-9d75-97ed3bc47830"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(aux_loss=True, backbone='resnet50', batch_size=1, bbox_loss_coef=5, cbackbone_layer=2, clip_max_norm=0.1, coco_panoptic_path=None, coco_path=None, content_loss_coef=0.1, dataset_file='demo', dec_layers=6, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dnorm=True, dropout=0.1, enc_layers=6, enorm=True, eos_coef=0.1, epochs=300, eval=False, fold_k=5, fold_stride=6, frozen_weights=None, giou_loss_coef=2, hidden_dim=256, img_size=408, in_content_folder='inputs/content', lr=1e-05, lr_backbone=1e-05, lr_drop=200, mask_loss_coef=1, masks=False, model_pre=False, model_type='nofold', nheads=8, num_queries=100, num_workers=1, output_dir='outputs', position_embedding='sine', pre_norm=False, remove_difficult=False, resume='outputs/nofold_noLN_contentMstyleM_dataN_enormTrue_dnormTrue_tnormTrue_cbl2_sbl4_batch1_k5_s6_lrb1e-05_lr1e-05_cw0.01_sw0.01_tw0_elyers6_dlyers6_h8_maxsize408/checkpoint/checkpoint0038.pth', sbackbone_layer=4, seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, style_folder='inputs/style', style_loss_coef=0.1, tnorm=True, tv_loss_coef=0, weight_decay=0.0001, wikiart_path=None, world_size=1)\n",
            "ISTT_NOFOLD(\n",
            "  (transformer): Transformer(\n",
            "    (encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (backbone_content): Backbone_50(\n",
            "    (body): IntermediateLayerGetter(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): FrozenBatchNorm2d()\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (backbone_style): Backbone_50(\n",
            "    (body): IntermediateLayerGetter(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): FrozenBatchNorm2d()\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d()\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d()\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d()\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d()\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (position_embedding): PositionEmbeddingSine()\n",
            "  (input_proj_c): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (input_proj_s): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (output_proj): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (tail): Sequential(\n",
            "    (0): ResBlock(\n",
            "      (net): Sequential(\n",
            "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.01)\n",
            "        (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.01)\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (7): LeakyReLU(negative_slope=0.01)\n",
            "        (8): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (1): Upsample(scale_factor=2.0, mode=bilinear)\n",
            "    (2): ReflectionPad2d((1, 1, 1, 1))\n",
            "    (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ResBlock(\n",
            "      (net): Sequential(\n",
            "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.01)\n",
            "        (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.01)\n",
            "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (7): LeakyReLU(negative_slope=0.01)\n",
            "        (8): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (5): Upsample(scale_factor=2.0, mode=bilinear)\n",
            "    (6): ReflectionPad2d((1, 1, 1, 1))\n",
            "    (7): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (8): ResBlock(\n",
            "      (net): Sequential(\n",
            "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (1): LeakyReLU(negative_slope=0.01)\n",
            "        (2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): LeakyReLU(negative_slope=0.01)\n",
            "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (7): LeakyReLU(negative_slope=0.01)\n",
            "        (8): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (9): Upsample(scale_factor=2.0, mode=bilinear)\n",
            "    (10): ReflectionPad2d((1, 1, 1, 1))\n",
            "    (11): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1))\n",
            "  )\n",
            ")\n",
            "number of params: 45197827\n",
            "len(content_images),len(style_images): 3 5\n",
            "inputs/content inputs/style\n",
            "15\n",
            "len(data_loader): 15\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3455: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "Test:  [ 0/15]  eta: 0:00:14  loss: 5.4310 (5.4310)  loss_content: 1.2146 (1.2146)  loss_style: 4.2164 (4.2164)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.1458 (12.1458)  loss_style_unscaled: 42.1638 (42.1638)  loss_tv_unscaled: 7933.8496 (7933.8496)  time: 0.9619  data: 0.1509  max mem: 4742\n",
            "Test:  [14/15]  eta: 0:00:00  loss: 4.1640 (4.3475)  loss_content: 1.2768 (1.3118)  loss_style: 3.0431 (3.0357)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.7679 (13.1183)  loss_style_unscaled: 30.4308 (30.3572)  loss_tv_unscaled: 9174.8613 (8108.5697)  time: 0.3739  data: 0.0146  max mem: 9318\n",
            "Test: Total time: 0:00:05 (0.3813 s / it)\n",
            "Averaged stats: loss: 4.1640 (4.3475)  loss_content: 1.2768 (1.3118)  loss_style: 3.0431 (3.0357)  loss_tv: 0.0000 (0.0000)  loss_content_unscaled: 12.7679 (13.1183)  loss_style_unscaled: 30.4308 (30.3572)  loss_tv_unscaled: 9174.8613 (8108.5697)\n",
            "max_mem: 9770924544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch\n",
        "!pip show torchvision"
      ],
      "metadata": {
        "id": "VkGHr0kDp0If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch -y\n",
        "!pip uninstall torchvision -y"
      ],
      "metadata": {
        "id": "M517I0gHqG6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install setuptools\n",
        "!pip install pycocotools\n",
        "!pip install pandas\n",
        "!pip install scipy\n",
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "OxpfMRGVqc8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm outputs/* -r"
      ],
      "metadata": {
        "id": "WcwZxW3P9TZN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "0jt8Hgs_rVDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3.7"
      ],
      "metadata": {
        "id": "aTlZZrNlrY5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1"
      ],
      "metadata": {
        "id": "0EltilcysAGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install python3-pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD0ynHJxsLyY",
        "outputId": "55733587-b0ec-44c6-8d5a-3a2f630680f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,677 kB of archives.\n",
            "After this operation, 8,967 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.1 [339 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.4 [1,305 kB]\n",
            "Fetched 1,677 kB in 0s (5,027 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3-setuptools.\n",
            "(Reading database ... 122397 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.4_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install --reinstall python3.7-distutils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODkwUIXisoYm",
        "outputId": "6d8a4edc-97f9-4510-f526-4af8d89a7316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3.7-lib2to3\n",
            "The following NEW packages will be installed:\n",
            "  python3.7-distutils python3.7-lib2to3\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 313 kB of archives.\n",
            "After this operation, 1,229 kB of additional disk space will be used.\n",
            "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-lib2to3 all 3.7.17-1+jammy1 [124 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-distutils all 3.7.17-1+jammy1 [189 kB]\n",
            "Fetched 313 kB in 2s (145 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3.7-lib2to3.\n",
            "(Reading database ... 123259 files and directories currently installed.)\n",
            "Preparing to unpack .../python3.7-lib2to3_3.7.17-1+jammy1_all.deb ...\n",
            "Unpacking python3.7-lib2to3 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7-distutils.\n",
            "Preparing to unpack .../python3.7-distutils_3.7.17-1+jammy1_all.deb ...\n",
            "Unpacking python3.7-distutils (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-lib2to3 (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-distutils (3.7.17-1+jammy1) ...\n"
          ]
        }
      ]
    }
  ]
}